---
sidebar_position: 30
---

# 应用与设计模式


| Type        |  Keypoints | Patterns   |
| ---------   | -------  |  --------  |
| <h4>News Feed</h4>  <ul><li>Twitter</li><li>Instagram</li><li>TikTok</li><li>Pinterest</li><li>RSS</li></ul>    | <ul><li>Use a fan-out / fan-in strategy for feed updates to enhance availability ([1](https://www.youtube.com/watch?v=WEgCjwyXvwc))</li><li>Read-Your-Writes Consistency ([1](https://arpitbhayani.me/blogs/read-your-write-consistency), [2](https://docs.oracle.com/cd/E17276_01/html/gsg_db_rep/C/rywc.html))</li><li>Maintain data replication across servers for consistency([1](https://www.youtube.com/watch?v=wapi0aR4BZE), [2](https://instagram-engineering.com/sharding-ids-at-instagram-1cf5a71e5a5c), [3](https://www.singlestore.com/blog/database-sharding-vs-partitioning-whats-the-difference/))</li></ul>   | <ul><li>Fan-Out / Fan-In Strategy: <ul><li>Distribute a new post to all follower feeds (fan-out); aggregate feeds when users log in (fan-in)</li></ul></li><li>Event Sourcing Pattern: <ul><li>Store changes to the news feed as events, ensuring all writes are recorded for user consistency</li><li>Replay events to rebuild user feed state in case of server failure, ensuring data consistency</li></ul></li><li>Sharding Pattern: <ul><li>Shard news feed data by user region to distribute load and reduce latency</li><li>Shard by user activity level, placing more active users on high-performance shards to balance load</li></ul></li></ul> |
| <h4>Share</h4> <ul><li>TinyURL</li><li>Bitly</li><li>Shorby</li><li>Pastebin</li></ul> | <ul><li>Pregenerate URLs for later use</li><li>Implement caching for frequent redirects to reduce latency ([1](https://docs.aws.amazon.com/whitepapers/latest/database-caching-strategies-using-redis/caching-patterns.html))</li><li>Apply rate limiting and validation to enhance security([1](https://freecontent.manning.com/all-about-bloom-filters/))</li></ul> | <ul><li>Cache-Aside Pattern: <ul><li>Cache frequently accessed short URLs to speed up redirect requests</li><li>Evict expired URLs from cache to ensure users are redirected correctly</li></ul></li><li>Rate Limiting Pattern: <ul><li>Limit URL creation requests to prevent abuse and ensure fair usage</li><li>Apply rate limiting on redirects to prevent DDoS attacks on popular URLs</li></ul></li><li>Publisher-Subscriber Pattern: <ul><li>Notify caching systems to evict or preload URLs when new ones are pregenerated</li></ul></li></ul> |
| <h4>GEO Base</h4>  <ul><li>Uber</li><li>Uber Eats</li><li>Grab</li><li>Lyft</li></ul> | <ul><li>Implement efficient matching algorithms for low latency（[1](https://medium.com/@bkawk/geohashing-20b282fc9655), [2](https://s2geometry.io/)）</li><li>Use GPS data for location tracking</li><li>Use log database to record events to facilitate data traceability</li></ul> | <ul><li>Geode Pattern: <ul><li>Use geodes to efficiently match riders with nearby drivers for low latency</li><li>Store and index GPS data geospatially for quick location-based queries</li></ul></li><li>Event Sourcing Pattern: <ul><li>Record all ride-related events for full traceability of the system's state</li><li>Use events to reconstruct a trip's state in case of disputes or audits</li></ul></li><li>Saga Distributed Transactions Pattern: <ul><li>Manage complex transactions like booking, payment, and confirmation across services</li><li>Ensure consistency in distributed systems when riders cancel or change routes</li></ul></li></ul> |
| <h4>IM</h4> <ul><li>WhatsApp</li><li>Facebook Messenger</li><li>WeChat</li><li>LINE</li></ul> | <ul><li>efficient messaging protocols like XMPP ([1](https://www.cometchat.com/blog/whatsapps-architecture-and-system-design))</li><li>Ensure data consistency with eventual consistency model</li><li>Encrypt messages for security ([1](https://signal.org/blog/how-to-build-encrypted-group-calls/))</li></ul> | <ul><li>Valet Key Pattern: <ul><li>Give clients temporary keys to upload/download attachments securely</li><li>Use keys to authenticate WebSocket/XMPP connections for sending/receiving messages</li></ul></li></ul> |
| <h4>Live</h4> <ul><li>Facebook Live</li><li>Twitch</li><li>Zoom</li><li>Google Meets</li></ul> | <ul><li>Optimize for real-time video delivery</li><li>Use scalable cloud services, CDN</li><li>Maintain stream quality with adaptive bitrate streaming using RTMPS and WebRTC ([1](https://www.dacast.com/blog/rtmps-streaming/))</li><li>Thundering herd problem ([1](https://engineering.fb.com/2015/12/03/ios/under-the-hood-broadcasting-live-video-to-millions/))</li></ul> | <ul><li>Sharding Pattern: <ul><li>Shard video streams across multiple servers for load balancing</li><li>Reduce latency by directing viewers to the nearest shard or CDN endpoint</li></ul></li><li>Cache-Aside Pattern: <ul><li>Cache popular streams and fetch from the origin server if not in cache</li><li>Update cache with the most viewed streams to handle sudden traffic spikes</li></ul></li><li>Circuit Breaker Pattern: <ul><li>Prevent system overloads during peak times by cutting off non-critical operations</li><li>Degrade service if a component fails to maintain overall system function</li></ul></li></ul> |
| <h4>Video</h4>  <ul><li>YouTube</li><li>Netflix</li><li>Disney Plus</li><li>HBO Max</li></ul>   | <ul><li>Use scalable cloud services like CDN</li><li>Implement adaptive bitrate streaming for dynamic network conditions</li><li>Use redundant systems, failover strategies</li><li>Architect for horizontal scalability</li></ul> | <ul><li>Sharding Pattern: <ul><li>Distribute video content across multiple servers to balance load</li><li>Serve users from the nearest data shard to reduce latency</li></ul></li><li>Cache-Aside Pattern: <ul><li>Store frequently accessed videos in cache to minimize read latency</li><li>Load data into the cache on demand, ensuring popular content is quickly accessible</li></ul></li><li>Circuit Breaker Pattern: <ul><li>Prevent cascading failures during peak loads by stopping operations to a failing service</li><li>Enable failover mechanisms to redirect traffic if a service or server fails</li></ul></li></ul>|
| <h4>File Sync</h4> <ul><li>Google Drive</li><li>Dropbox</li><li>Sync.Com</li><li>iDrive</li></ul>   | <ul><li>Implement efficient file transfer protocols like Rsync algorithm ([1](https://dropbox.tech/infrastructure/rewriting-the-heart-of-our-sync-engine))</li><li>Use asynchronous and chunked file uploading for efficiency ([1](https://dropbox.tech/infrastructure/boosting-dropbox-upload-speed))</li><li>Shift to microservices improved latency, resiliency, and resource efficiency([1](https://netflixtechblog.com/rebuilding-netflix-video-processing-pipeline-with-microservices-4e5e6310e359))</li></ul>  | <ul><li>Asynchronous Request-Reply Pattern: <ul><li>Handle file uploads where the client initiates a request and is free to perform other tasks while waiting for the response</li><li>Process large file downloads in the background</li></ul></li><li>CQRS Pattern: <ul><li>Separate read requests (retrieving file lists) from write requests (uploading files) for optimized performance</li><li>Allow quick access to file metadata while ensuring consistent writes during uploads</li></ul></li><li>Retry Pattern & Circuit Breaker Pattern: <ul><li>Retry file transfers when transient network issues occur</li><li>Open circuit breaker to prevent a failing file transfer service from being overwhelmed</li></ul></li></ul> |
| <h4>Search</h4>  <ul><li>Google Search</li><li>Bing</li><li>Airbnb Search</li><li>Twitter Search</li></ul> | <ul><li>Implement Inverted Index for efficient keyword matching ([1](https://www.youtube.com/watch?v=0eKVizvYSUQ))</li><li>Personalize search results using user history and PageRank</li><li>Optimize search algorithms for speed and accuracy</li></ul> | <ul><li>Sharding Pattern: <ul><li>Shard the search index by keyword ranges to distribute the load across servers</li><li>Parallelize web pages across shards to enhance performance and scalability</li></ul></li><li>Cache-Aside Pattern: <ul><li>Cache popular search results and serve them quickly for repeated queries</li><li>Update the cache proactively when frequent keywords are queried to reduce latency</li></ul></li><li>Publisher-Subscriber Pattern: <ul><li>Publish search queries to a topic, with subscribers updating personalized ranking asynchronously</li><li>Use subscribers to trigger index updates when new content is available or changed</li></ul></li></ul>|
| <h4>Ranking</h4> <ul><li>Twitter Top Hashtags</li><li>Spotify Top K Music</li></ul>  | <ul><li>Design for scalability to handle complex, multi-dimensional queries</li><li>Develop a caching strategy for frequently accessed filtered results</li><li>Pre-compute rankings to meet specific business timeframes for updates</li><li>Batch multiple update transactions into a single operation to optimize performance ([1](https://engineering.atspotify.com/2016/02/spotifys-event-delivery-the-road-to-the-cloud-part-i/))</li><li>Employ Lossy Counting and Global Query Buckets for efficient, approximate data aggregation</li></ul> | <ul><li>Cache-Aside Pattern: <ul><li>Cache top tags or tracks to quickly serve frequent queries without hitting the database</li></ul></li><li>Sharding Pattern: <ul><li>Shard data by tags or music genres to distribute query load and improve scalability</li><li>Allow parallel processing of multi-dimensional queries across different shards</li></ul></li><li>Fan-Out/Fan-In Strategy: <ul><li>Fan-out updates to pre-computed rankings across multiple systems, then fan-in for a global view</li><li>Aggregate data from multiple shards (fan-in) for complex queries to generate final results</li></ul></li></ul>|
| <h4>Trading</h4>  <ul><li>Robinhood</li><li>Webull</li><li>TradeStation</li></ul>   |  <ul><li>Implement efficient data streaming services for low-latency market updates</li><li>Ensure reliable trade execution with minimal processing delay</li><li>Integrate timely financial news delivery and custom alerting features</li><li>Facilitate secure and compliant fund transfer mechanisms</li></ul> | <ul><li>CQRS Pattern: <ul><li>Separate read queries for financial news delivery from write operations for trade execution</li><li>Streamline command processing to reduce trade execution latency</li></ul></li><li>Valet Key Pattern: <ul><li>Secure fund transfers by providing temporary credentials for transactional operations</li><li>Minimize data access scope using valet keys for third-party financial news integration</li></ul></li></ul> |
| <h4>Cooperative editing</h4> <ul><li>Google Docs</li><li>Microsoft Office Online</li><li>Quip</li></ul>   | <ul><li>Implement operational transformation or CRDTs for low-latency, consistent collaborative editing ([1](https://drive.googleblog.com/2010/05/whats-different-about-new-google-docs.html))</li><li>Maintain document consistency with version control and conflict resolution mechanisms ([1](https://drive.googleblog.com/2010/09/whats-different-about-new-google-docs.html))</li><li>Design for scalability to handle simultaneous edits by thousands of users</li><li>Optimize for multi-device compatibility, ensuring consistent user experience</li></ul> | <ul><li>Event Sourcing Pattern: <ul><li>Record each edit as an event, enabling operational transformation for collaborative editing</li><li>Replay events to resolve conflicts and maintain document version history</li></ul></li><li>Compensating Transaction Pattern: <ul><li>Reverse an edit operation when a conflict is detected to maintain consistency</li><li>Apply compensating transactions to handle operational transformation conflicts</li></ul></li></ul> |
| <h4>Booking</h4> <ul><li>Google Calendar</li><li>Ticketmaster</li><li>StubHub</li></ul>  | <ul><li>Employ distributed transactions, possibly with XA protocol, for cross-service operations</li><li>Prevent write skew and phantoms using serializable isolation levels in databases</li></ul>  | <ul><li>Saga Distributed Transactions Pattern: <ul><li>Coordinate event bookings across multiple services without XA protocol overhead</li><li>Handle failures in one service by executing compensating transactions in others</li></ul></li><li>Event Sourcing Pattern: <ul><li>Store state changes as events to prevent write skew in distributed transactions</li><li>Replay events to rebuild system state for serializable isolation level enforcement</li></ul></li></ul> |
| <h4>Shopping</h4> <ul><li>Amazon</li><li>eBay</li><li>Etsy</li><li>AliExpress</li></ul> | <ul><li>Implement elastic search for efficient, scalable product discovery</li><li>Employ event sourcing for reliable order tracking and management</li><li>Secure user accounts with OAuth and multi-factor authentication</li><li>Utilize a microservices architecture to ensure scalability of user and order systems</li></ul> | <ul><li>Event Sourcing Pattern: <ul><li>Record order events to track status changes</li><li>Enable modifying orders by replaying and adjusting events</li></ul></li><li>Cache-Aside Pattern: <ul><li>Cache product listings and details</li><li>Invalidate cache on product updates to ensure users see current information</li></ul></li></ul> | 
| <h4>CPU-bound</h4> <ul><li>PDF converter</li><li>Video Editor</li></ul>  | <ul><li>Serverless architecture to reduce system complexity</li><li>Utilize divide and conquer, map-reduce strategies to break down tasks</li><li>Manage variable workloads with auto-scaling cloud services</li></ul> | <ul><li>Fan-Out/Fan-In Strategy: <ul><li>Distribute PDF conversion tasks across multiple functions</li><li>Apply video effects in parallel, then merge clips into a final video</li></ul></li><li>Asynchronous Request-Reply Pattern: <ul><li>Queue PDF conversion jobs and notify users asynchronously when done</li><li>Submit video rendering jobs and provide results via callback or webhook</li></ul></li><li>Health Endpoint Monitoring Pattern: <ul><li>Monitor health of serverless functions to ensure availability for video editing</li><li>Track PDF converter service health to scale or redeploy functions</li></ul></li></ul> |
| <h4>IoT</h4> <ul><li>Fitbit</li><li>Nest</li></ul>  | <ul><li>Utilize event sourcing to capture and store each user interaction and device event</li><li>Implement efficient data compression / protocol for low-latency synchronization like MQTT</li><li>Employ eventual consistency model for user data synchronization</li></ul> | <ul><li>Event Sourcing Pattern: <ul><li>Record data as events for historical analysis</li><li>Capture device temperature changes to recreate historical states for troubleshooting</li></ul></li><li>Publisher-Subscriber Pattern: <ul><li>Devices publish sensor data, which apps subscribe to for real-time updates</li><li>Subscribe to user preferences changes to update device behavior dynamically</li></ul></li><li>Sharding Pattern: <ul><li>Shard data by user ID for scalable, distributed data storage</li><li>Use geographical sharding to reduce latency for location-based IoT devices</li></ul></li></ul> | 
| <h4>Infrastructure</h4> <ul><li>Message queue</li><li>Object Storage</li><li>Logging</li><li>Monitoring</li></ul> |  <ul><li>Use partitioning for scalable throughput</li><li>Replicate data for high availability</li><li>Implement consistent hashing for consistency</li><li>Encrypt data at rest for security</li></ul> | <ul><li>Health Endpoint Monitoring Pattern: <ul><li> Monitor the health of nodes, ensuring consistent message processing</li><li>Check object storage service health to detect and resolve access issues</li></ul></li><li>Event Sourcing Pattern: <ul><li>Store every change in system configuration as an event</li><li>Record log generation and access events to reconstruct system states during incidents</li></ul></li></ul> |

